@inproceedings{Ren2022,
abstract = {The improvement of temporal resolution of hyperspectral (HS) data is a fundamental and challenging problem. In this paper, we propose a Temporal-Spectral fusion method based on Generative Adversarial Network (TSF-GAN). First, the generator is used to train the nonlinear relationship between multispectral (MS) and HS data pairs at time T1 and T3, and we map the relationship to the MS data at T2 to obtain the HS data. Second, the discriminator is used to identify whether the differential image of HS data at different times is consistent with that of MS data, and whether the HS data at time T2 after spectral down-sampling is consistent with that of MS data at time T2. Preliminary experimental results demonstrate that the proposed TSF-GAN achieves comparative fidelity and has strong practicability.},
author = {Ren, Kai and Sun, Weiwei and Zhou, Jun and Meng, Xiangchao and Yang, Gang and Peng, Jiangtao},
booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
doi = {10.1109/IGARSS46834.2022.9884778},
mendeley-groups = {Case Study - FusionGAN},
title = {{A Temporal-Spectral Generative Adversarial Fusion Network for Improving Satellite Hyperspectral Temporal Resolution}},
volume = {2022-July},
year = {2022}
}
@article{Lu2017,
abstract = {Forest plays an important role in global carbon, hydrological and atmospheric cycles and provides a wide range of valuable ecosystem services. Timely and accurate forest-type mapping is an essential topic for forest resource inventory supporting forest management, conservation biology and ecological restoration. Despite efforts and progress having been made in forest cover mapping using multi-source remotely sensed data, fine spatial, temporal and spectral resolution modeling for forest type distinction is still limited. In this paper, we proposed a novel spatial-temporal-spectral fusion framework through spatial-spectral fusion and spatial-temporal fusion. Addressing the shortcomings of the commonly-used spatial-spectral fusion model, we proposed a novel spatial-spectral fusion model called the Segmented Difference Value method (SEGDV) to generate fine spatial-spectra-resolution images by blending the China environment 1A series satellite (HJ-1A) multispectral image (Charge Coupled Device (CCD)) and Hyperspectral Imager (HSI). A Hierarchical Spatiotemporal Adaptive Fusion Model (HSTAFM) was used to conduct spatial-temporal fusion to generate the fine spatial-temporal-resolution image by blending the HJ-1A CCD and Moderate Resolution Imaging Spectroradiometer (MODIS) data. The spatial-spectral-temporal information was utilized simultaneously to distinguish various forest types. Experimental results of the classification comparison conducted in the Gan River source nature reserves showed that the proposed method could enhance spatial, temporal and spectral information effectively, and the fused dataset yielded the highest classification accuracy of 83.6% compared with the classification results derived from single Landsat-8 (69.95%), single spatial-spectral fusion (70.95%) and single spatial-temporal fusion (78.94%) images, thereby indicating that the proposed method could be valid and applicable in forest type classification.},
author = {Lu, Ming and Chen, Bin and Liao, Xiaohan and Yue, Tianxiang and Yue, Huanyin and Ren, Shengming and Li, Xiaowen and Nie, Zhen and Xu, Bing},
doi = {10.3390/rs9111153},
issn = {20724292},
journal = {Remote Sensing},
mendeley-groups = {Case Study - FusionGAN},
number = {11},
title = {{Forest types classification based on multi-source data fusion}},
volume = {9},
year = {2017}
}
@article{Zhu2023,
abstract = {Multispectral and hyperspectral image fusion (MHIF) involves the fusion of high-spatial-resolution multispectral images (HR-MSIs) and low-spatial-resolution hyperspectral images (LR-HSIs) to generate high-spatial-resolution hyperspectral images (HR-HSIs) and has gained significant attention in the field of remote-sensing imaging. While CNN and Transformer models have shown effectiveness in MHIF, existing CNN- or Transformer-based algorithms are overburdened with model size, making it difficult to achieve an effective tradeoff between fusion accuracy and degree of lightweight. Recently, implicit neural representation (INR) has been proven good interpretability and the ability to exploit coordinate information in 2-D tasks. Nonetheless, INR-based fusion networks have certain limitations, such as the need for deeper super-resolution networks as shallow encoders, and insufficient representation capability on high upsampling ratios. To address these challenges, we present the quadtree implicit sampling (QIS), which employs a hierarchical sampling from the perspective of the quadtree, to enhance the capacity of the overall network. Furthermore, the remarkable design of QIS allows us to adopt a lightweight structure as the shallow encoder, greatly alleviating the network burden and achieving lightweight. Inspired by generative adversarial models, we incorporate QIS as a lightweight generator into the generative adversarial network (GAN) framework named QIS-GAN and leverage a discriminator to increase the fidelity of fused images. The results showcase the superior performance of QIS-GAN on the MHIF tasks with upsampling ratios of × 4 , × 8 , and × 16 , surpassing the state-of-the-art (SOTA) in several datasets. The code for our approach will be available at https://github.com/chunyuzhu/QIS-GAN.},
author = {Zhu, Chunyu and Deng, Shangqi and Zhou, Yingjie and Deng, Liang Jian and Wu, Qiong},
doi = {10.1109/TGRS.2023.3332176},
file = {:E\:/Advaith C A/Vault/GAN for RS/QIS-GAN_A_Lightweight_Adversarial_Network_With_Quadtree_Implicit_Sampling_for_Multispectral_and_Hyperspectral_Image_Fusion.pdf:pdf},
issn = {15580644},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
mendeley-groups = {Case Study - FusionGAN},
title = {{QIS-GAN: A Lightweight Adversarial Network with Quadtree Implicit Sampling for Multispectral and Hyperspectral Image Fusion}},
volume = {61},
year = {2023}
}
@article{Zhang2022,
abstract = {Leaf area index (LAI) is a key indicator for the growth of crops. The crop yield is also closely related to the LAI, particularly for the decision-making in modern agriculture. Rapid and accurate detection of the crop LAI is of great significance to field production management. Single sensors have been mostly used to monitor the winter wheat LAI in the past, such as high-definition digital cameras, multispectral and hyperspectral cameras. Fortunately, Unmanned Aerial Vehicles (UAV) remote sensing technology has been developed for crop LAI monitoring by virtue of the high timeliness and low cost at present. The multi-source image data can also be combined for parameter monitoring. In addition, previous LAI estimation is limited to either only the correlation of image features with LAI, or only the importance of image features. Therefore, this study aims to comprehensively consider the correlation of image feature with the LAI and image feature importance, and then construct the multi-source remote sensing LAI estimation models using the selection of optimal image features. Baihu Farm in Lujiang County and Shucheng County Agricultural Science Institute in Anhui Province of China were selected as the study areas, where the canopy visible and hyperspectral images of winter wheat at flowering and filling stages were collected by a UAV platform equipped with a high-definition digital camera and an imaging hyperspectrometer. Meanwhile, the ground LAI data was collected using LAI-2200C. The multiple linear regression (MLR), support vector regression (SVR), and random forest regression (RFR) algorithms were selected to estimate the wheat LAI using the visible and hyperspectral image data. Firstly, the correlation between the visible and hyperspectral image features with the winter wheat LAI was analyzed as well as the importance of image features were calculated, where the optimal image features were selected. Secondly, the MLR, SVR, and RFR estimation models (single-sensor data sources) were constructed, where the input was taken as the visible vegetation index, the texture features, visible vegetation index combined with texture features, hyperspectral band, hyperspectral vegetation index, and hyperspectral band combined with vegetation index. The RFR and SVR LAI estimation models (two sensor data sources) were constructed with two image-preferred features to compare the performance of single-source and multi-source image features for the monitoring of wheat LAI. Thirdly, the spatial heterogeneity of plot soils was considered for the wheat LAI monitoring. The wheat LAI estimation model was also constructed to combine the single image features under different image sampling areas. The results showed that the best accuracy of the RFR LAI estimation model was achieved at the flowering and filling stages using two image preferred features, with the validation set R2 of 0.733 and 0.929 and RMSE of 0.193 and 0.118, respectively. By contrast, the model using the combination of single image features performed the best at the flowering and filling stages, when the sampling areas of visible images were 30% and 50%, and the sampling areas of hyperspectral images were 65%, respectively. In summary, the study can provide a valuable reference for the UAV remote sensing monitoring of crop physiological parameters.},
author = {Zhang, Dongyan and Han, Xuanxuan and Lin, Fenfang and Du, Shizhou and Zhang, Gan and Hong, Qi},
doi = {10.11975/j.issn.1002-6819.2022.09.018},
issn = {10026819},
journal = {Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
mendeley-groups = {Case Study - FusionGAN},
number = {9},
title = {{Estimation of winter wheat leaf area index using multi-source UAV image feature fusion}},
volume = {38},
year = {2022}
}
@article{Zhu2023a,
abstract = {Hyperspectral remote sensing image (HSI) fusion with multispectral remote sensing images (MSI) improves data resolution. However, current fusion algorithms focus on local information and overlook long-range dependencies. The parameter of network tuning prioritizes global optimization, neglecting spatial and spectral constraints, and limiting spatial and spectral reconstruction capabilities. This study introduces SwinGAN, a fusion network combining Swin Transformer, CNN, and GAN architectures. SwinGAN's generator employs a detail injection framework to separately extract HSI and MSI features, fusing them to generate spatial residuals. These residuals are injected into the supersampled HSI to produce the final image, while a pure CNN architecture acts as the discriminator, enhancing the fusion quality. Additionally, we introduce a new adaptive loss function that improves image fusion accuracy. The loss function uses L1 loss as the content loss, and spatial and spectral gradient loss functions are introduced to improve the spatial representation and spectral fidelity of the fused images. Our experimental results on several datasets demonstrate that SwinGAN outperforms current popular algorithms in both spatial and spectral reconstruction capabilities. The ablation experiments also demonstrate the rationality of the various components of the proposed loss function.},
author = {Zhu, Chunyu and Deng, Shangqi and Li, Jiaxin and Zhang, Ying and Gong, Liwei and Gao, Liangbo and Ta, Na and Chen, Shengbo and Wu, Qiong},
doi = {10.1080/17538947.2023.2253206},
issn = {17538955},
journal = {International Journal of Digital Earth},
mendeley-groups = {Case Study - FusionGAN},
number = {1},
title = {{Hyperspectral and multispectral remote sensing image fusion using SwinGAN with joint adaptive spatial-spectral gradient loss function}},
volume = {16},
year = {2023}
}
@article{Goodfellow2020,
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1145/3422622},
issn = {15577317},
journal = {Communications of the ACM},
mendeley-groups = {Case Study - FusionGAN},
number = {11},
title = {{Generative adversarial networks}},
volume = {63},
year = {2020}
}
@article{Su2023,
abstract = {Hyperspectral images (HSI) frequently have inadequate spatial resolution, which hinders numerous applications for the images. High resolution multispectral image (MSI) has been fused with HSI to reconstruct images with both high spatial and high spectral resolutions. In this paper, we propose a generative adversarial network (GAN)-based unsupervised HSI-MSI fusion network. In the generator, two coupled autoencoder nets decompose HSI and MSI into endmembers and abundances for fusing high resolution HSI through the linear mixing model. The two autoencoder nets are connected by a degradation-generation (DG) block, which further improves the accuracy of the reconstruction. Additionally, a coordinate multi-attention net (CMAN) is designed to extract more detailed features from the input. Driven by the joint loss function, the proposed method is straightforward and easy to execute in an end-to-end training manner. The experimental results demonstrate that the proposed strategy outperforms the state-of-art methods.},
author = {Su, Lijuan and Sui, Yuxiao and Yuan, Yan},
doi = {10.3390/rs15040936},
file = {:C\:/Users/mrg23-021419013/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su, Sui, Yuan - 2023 - An Unmixing-Based Multi-Attention GAN for Unsupervised Hyperspectral and Multispectral Image Fusion.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {GAN,hyperspectral image (HSI),image fusion,multi-attention mechanism},
mendeley-groups = {Case Study - FusionGAN},
number = {4},
title = {{An Unmixing-Based Multi-Attention GAN for Unsupervised Hyperspectral and Multispectral Image Fusion}},
volume = {15},
year = {2023}
}
@article{Xiao2021,
abstract = {Hyperspectral image (HSI) fusion can effectively improve the spatial resolution of HSIs by integrating high-resolution multispectral images (MSIs). Considering the spatial and spectral degradation relationship between a fused image and input images, a physics-based GAN is proposed to fuse HSI and MSI. A physical model estimating degradation of image is introduced in the generator and in the discriminators. For the generator, a set of recursive modules including a physical degradation model and a multiscale residual channel attention fusion module integrate the spectral-spatial difference information between input images and estimated degradation images to restore the details of the fused image. Subsequently, the residual spatial attention fusion module is used to combine the results of all recursions to obtain the final reconstructed result. As for the discriminators, three networks with the final fused image, estimated LR HSI and estimated MSI as inputs share the same architecture. Finally, the loss function that contains adversarial losses and L1 losses of the fused image and estimated degradation images is used to optimize network parameters. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods.},
author = {Xiao, Jiajun and Li, Jie and Yuan, Qiangqiang and Jiang, Menghui and Zhang, Liangpei},
doi = {10.1109/JSTARS.2021.3075727},
file = {:C\:/Users/mrg23-021419013/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao et al. - 2021 - Physics-Based GAN with Iterative Refinement Unit for Hyperspectral and Multispectral Image Fusion.pdf:pdf},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
mendeley-groups = {Case Study - FusionGAN},
title = {{Physics-Based GAN with Iterative Refinement Unit for Hyperspectral and Multispectral Image Fusion}},
volume = {14},
year = {2021}
}
